{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport wordcloud\nimport nltk\nimport re\nimport string         \nimport keras\nimport tensorflow as tf\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom textblob import Word\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, SpatialDropout2D, LSTM\nfrom keras.callbacks import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-03T19:51:25.020293Z","iopub.execute_input":"2022-12-03T19:51:25.020666Z","iopub.status.idle":"2022-12-03T19:51:25.03083Z","shell.execute_reply.started":"2022-12-03T19:51:25.020633Z","shell.execute_reply":"2022-12-03T19:51:25.029761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read in data\ndf = pd.read_csv('../input/amazon-fine-food-reviews/Reviews.csv')\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:25.042758Z","iopub.execute_input":"2022-12-03T19:51:25.043059Z","iopub.status.idle":"2022-12-03T19:51:28.612702Z","shell.execute_reply.started":"2022-12-03T19:51:25.043033Z","shell.execute_reply":"2022-12-03T19:51:28.611741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:28.614811Z","iopub.execute_input":"2022-12-03T19:51:28.615177Z","iopub.status.idle":"2022-12-03T19:51:28.62955Z","shell.execute_reply.started":"2022-12-03T19:51:28.61514Z","shell.execute_reply":"2022-12-03T19:51:28.628386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:28.631178Z","iopub.execute_input":"2022-12-03T19:51:28.631988Z","iopub.status.idle":"2022-12-03T19:51:28.712915Z","shell.execute_reply.started":"2022-12-03T19:51:28.631952Z","shell.execute_reply":"2022-12-03T19:51:28.711863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:28.715581Z","iopub.execute_input":"2022-12-03T19:51:28.717057Z","iopub.status.idle":"2022-12-03T19:51:28.894706Z","shell.execute_reply.started":"2022-12-03T19:51:28.717017Z","shell.execute_reply":"2022-12-03T19:51:28.893672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the information above:\n- we have no null values to worry about, so no missing values\n- we have two type of columns, either int64 or object, in other word strings.\n- we will focus on the score, summary and text column so we can drop the rest","metadata":{}},{"cell_type":"code","source":"all_cols = df.columns\nkeep_cols = ['Score', 'Summary', 'Text']\ndf.drop([c for c in all_cols if c not in keep_cols], axis=1, inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:28.896904Z","iopub.execute_input":"2022-12-03T19:51:28.897689Z","iopub.status.idle":"2022-12-03T19:51:28.929701Z","shell.execute_reply.started":"2022-12-03T19:51:28.897645Z","shell.execute_reply":"2022-12-03T19:51:28.92882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score distribution before aggregation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.title('Score distribution')\nsns.histplot(df['Score'], discrete=True);","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:28.931004Z","iopub.execute_input":"2022-12-03T19:51:28.931611Z","iopub.status.idle":"2022-12-03T19:51:29.475655Z","shell.execute_reply.started":"2022-12-03T19:51:28.93157Z","shell.execute_reply":"2022-12-03T19:51:29.474761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a new column ‘sentiment’ based on ‘Score’\n","metadata":{}},{"cell_type":"code","source":"def sentiments(df):\n    return 'Positive' if (df['Score'] > 3) else 'Negative'\ndf['sentiment'] = df.apply(sentiments, axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:29.477618Z","iopub.execute_input":"2022-12-03T19:51:29.478302Z","iopub.status.idle":"2022-12-03T19:51:34.74082Z","shell.execute_reply.started":"2022-12-03T19:51:29.478264Z","shell.execute_reply":"2022-12-03T19:51:34.739753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['Score'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:34.744212Z","iopub.execute_input":"2022-12-03T19:51:34.744498Z","iopub.status.idle":"2022-12-03T19:51:34.825672Z","shell.execute_reply.started":"2022-12-03T19:51:34.744471Z","shell.execute_reply":"2022-12-03T19:51:34.8247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot score distribution after aggregation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.title('Sentiment distribution')\nsns.histplot(df['sentiment']);","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:34.827085Z","iopub.execute_input":"2022-12-03T19:51:34.827556Z","iopub.status.idle":"2022-12-03T19:51:35.65895Z","shell.execute_reply.started":"2022-12-03T19:51:34.827513Z","shell.execute_reply":"2022-12-03T19:51:35.658067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the data is highly imbalanced toward positive reviews so we need to be careful when splitting the dataset into training and testing datasets.","metadata":{}},{"cell_type":"markdown","source":"## Combine columns Summary with Text into full_text","metadata":{}},{"cell_type":"code","source":"df['full_text'] = df['Summary'] + '. ' + df['Text']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:35.664323Z","iopub.execute_input":"2022-12-03T19:51:35.664984Z","iopub.status.idle":"2022-12-03T19:51:36.167692Z","shell.execute_reply.started":"2022-12-03T19:51:35.664945Z","shell.execute_reply":"2022-12-03T19:51:36.166713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['Summary', 'Text'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:36.169222Z","iopub.execute_input":"2022-12-03T19:51:36.169576Z","iopub.status.idle":"2022-12-03T19:51:36.316679Z","shell.execute_reply.started":"2022-12-03T19:51:36.16954Z","shell.execute_reply":"2022-12-03T19:51:36.315683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean the text","metadata":{}},{"cell_type":"markdown","source":"Data cleaning involves deleting special letters, digits, irrelevant symbols, and stop words. It is also necessary to translate the terms to their root form for easier interpretation.","metadata":{}},{"cell_type":"code","source":"def replace_contractions(s):\n    #dictionary consisting of the contraction and the actual value\n    Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n               \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n\n    #replace the contractions\n    for key,value in Apos_dict.items():\n        if key in s:\n            s=s.replace(key,value)\n    return s","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:36.318334Z","iopub.execute_input":"2022-12-03T19:51:36.318748Z","iopub.status.idle":"2022-12-03T19:51:36.324946Z","shell.execute_reply.started":"2022-12-03T19:51:36.318711Z","shell.execute_reply":"2022-12-03T19:51:36.323813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation(s, punct_list):\n    for punc in punct_list:\n        if punc in s:\n            s = s.replace(punc, ' ')\n    return s.strip()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:36.326581Z","iopub.execute_input":"2022-12-03T19:51:36.326957Z","iopub.status.idle":"2022-12-03T19:51:36.335346Z","shell.execute_reply.started":"2022-12-03T19:51:36.326921Z","shell.execute_reply":"2022-12-03T19:51:36.334496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncate_large_review(s, seq_length):\n    ''' Return a truncated s to the input seq_length.\n    '''    \n    review_len = len(s)\n        \n    if review_len > seq_length:\n        return s[0:seq_length]\n    return s","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:36.336814Z","iopub.execute_input":"2022-12-03T19:51:36.337205Z","iopub.status.idle":"2022-12-03T19:51:36.344526Z","shell.execute_reply.started":"2022-12-03T19:51:36.337146Z","shell.execute_reply":"2022-12-03T19:51:36.343539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaning(df):\n    # make text lowercase\n    df['full_text'] = df['full_text'].apply(lambda s: str(s).lower())\n    print('To lowercase is done')\n    \n    # replace contractions\n    df['full_text'] = df['full_text'].apply(lambda s: replace_contractions(s))\n    print('Contractions replacement is done')\n    \n    # remove html tags\n    df['full_text'] = df['full_text'].apply(lambda s: re.compile(r'<[^>]+>').sub('', s))\n    print('HTML tags removal is done')\n    \n    # remove punctuation\n    regular_punct = list(string.punctuation)\n    df['full_text'] = df['full_text'].apply(lambda s: remove_punctuation(s, regular_punct))\n    print('Punctuation removal is done')\n    \n    # split attached words\n    df['full_text'] = df['full_text'].apply(lambda s: \" \".join([x for x in re.split(\"([A-Z][a-z]+[^A-Z]*)\",s) if x]))\n    print('Splitting attached words is done')\n    \n    # Replacing the digits/numbers\n    df['full_text'] = df['full_text'].apply(lambda s: re.sub(r'\\d+', '', s))\n    print('Numbers replacement is done')\n    \n    # truncate large review\n    df['full_text'] = df['full_text'].apply(lambda s: truncate_large_review(s, 300))\n    print('Truncation of large reviews is done')\n\n    return df\n\ndf = cleaning(df)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:36.346022Z","iopub.execute_input":"2022-12-03T19:51:36.34641Z","iopub.status.idle":"2022-12-03T19:51:51.13051Z","shell.execute_reply.started":"2022-12-03T19:51:36.346356Z","shell.execute_reply":"2022-12-03T19:51:51.128756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_words=''\nfor i in df.full_text:\n    i = str(i)\n    tokens = i.split()\n    common_words += \" \".join(tokens)+\" \"\nwordcloud = wordcloud.WordCloud().generate(common_words)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:51:51.131865Z","iopub.execute_input":"2022-12-03T19:51:51.132562Z","iopub.status.idle":"2022-12-03T19:53:10.010067Z","shell.execute_reply.started":"2022-12-03T19:51:51.132519Z","shell.execute_reply":"2022-12-03T19:53:10.008866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:10.015501Z","iopub.execute_input":"2022-12-03T19:53:10.016094Z","iopub.status.idle":"2022-12-03T19:53:10.030756Z","shell.execute_reply.started":"2022-12-03T19:53:10.016043Z","shell.execute_reply":"2022-12-03T19:53:10.0293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see in the word cloud that most reviews are positive since one of the biggest word is love, and we see words like good, great, delicious, etc. We can also see that the dataset is related to food since we see words like taste, coffee, eat, etc. ","metadata":{}},{"cell_type":"markdown","source":"## Splitting the dataset into training and testing set","metadata":{}},{"cell_type":"markdown","source":"- Since the majority of reviews are positive (5 stars), we will need to do a stratified split on the reviews score to ensure that we don’t train the classifier on imbalanced data\n- We are going to use sklearn’s Stratified ShuffleSplit class","metadata":{}},{"cell_type":"code","source":"# Get the split indexes\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=1,\n                                          test_size=0.3,\n                                          random_state=42)\ntrain_idx, test_idx = next(strat_shuf_split.split(df.full_text, df.sentiment))\n\n# Create the dataframes\nX_train = df.loc[train_idx, 'full_text']\ny_train = df.loc[train_idx, 'sentiment']\n\nX_test = df.loc[test_idx, 'full_text']\ny_test = df.loc[test_idx, 'sentiment']","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:10.03236Z","iopub.execute_input":"2022-12-03T19:53:10.033249Z","iopub.status.idle":"2022-12-03T19:53:10.513728Z","shell.execute_reply.started":"2022-12-03T19:53:10.033215Z","shell.execute_reply":"2022-12-03T19:53:10.512759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:10.515062Z","iopub.execute_input":"2022-12-03T19:53:10.515512Z","iopub.status.idle":"2022-12-03T19:53:10.540836Z","shell.execute_reply.started":"2022-12-03T19:53:10.515475Z","shell.execute_reply":"2022-12-03T19:53:10.539899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:10.542335Z","iopub.execute_input":"2022-12-03T19:53:10.544787Z","iopub.status.idle":"2022-12-03T19:53:10.559899Z","shell.execute_reply.started":"2022-12-03T19:53:10.544758Z","shell.execute_reply":"2022-12-03T19:53:10.558617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the features and encoding the target","metadata":{}},{"cell_type":"code","source":"max_features = 20000\nseq_length = 300  # How long to make our word sequences\nbatch_size = 1000","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:10.561841Z","iopub.execute_input":"2022-12-03T19:53:10.562416Z","iopub.status.idle":"2022-12-03T19:53:10.570436Z","shell.execute_reply.started":"2022-12-03T19:53:10.562185Z","shell.execute_reply":"2022-12-03T19:53:10.569438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:10.571947Z","iopub.execute_input":"2022-12-03T19:53:10.572423Z","iopub.status.idle":"2022-12-03T19:53:25.792063Z","shell.execute_reply.started":"2022-12-03T19:53:10.572383Z","shell.execute_reply":"2022-12-03T19:53:25.791031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:25.793479Z","iopub.execute_input":"2022-12-03T19:53:25.793868Z","iopub.status.idle":"2022-12-03T19:53:44.608392Z","shell.execute_reply.started":"2022-12-03T19:53:25.793824Z","shell.execute_reply":"2022-12-03T19:53:44.607373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:44.610003Z","iopub.execute_input":"2022-12-03T19:53:44.610389Z","iopub.status.idle":"2022-12-03T19:53:44.620368Z","shell.execute_reply.started":"2022-12-03T19:53:44.610341Z","shell.execute_reply":"2022-12-03T19:53:44.619322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = pad_sequences(sequences_train, maxlen=seq_length)\nx_test = pad_sequences(sequences_test, maxlen=seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:44.622173Z","iopub.execute_input":"2022-12-03T19:53:44.622951Z","iopub.status.idle":"2022-12-03T19:53:47.393643Z","shell.execute_reply.started":"2022-12-03T19:53:44.622908Z","shell.execute_reply":"2022-12-03T19:53:47.392641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(y_train)\n\ny_train = encoder.transform(y_train)\ny_test = encoder.transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:47.395113Z","iopub.execute_input":"2022-12-03T19:53:47.395472Z","iopub.status.idle":"2022-12-03T19:53:47.510494Z","shell.execute_reply.started":"2022-12-03T19:53:47.395436Z","shell.execute_reply":"2022-12-03T19:53:47.509572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use Glove pre-trained word vectors","metadata":{}},{"cell_type":"code","source":"embeddings_index = {}\nf = open('../input/glovedata/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:47.512047Z","iopub.execute_input":"2022-12-03T19:53:47.512413Z","iopub.status.idle":"2022-12-03T19:53:54.573361Z","shell.execute_reply.started":"2022-12-03T19:53:47.512379Z","shell.execute_reply":"2022-12-03T19:53:54.572261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## This creates a matrix where the $i$th row gives the word embedding for the word represented by integer $i$.\n## Essentially, these will be the \"weights\" for the Embedding Layer\n## Rather than learning the weights, we will use these ones and \"freeze\" the layer\n\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:54.578512Z","iopub.execute_input":"2022-12-03T19:53:54.578817Z","iopub.status.idle":"2022-12-03T19:53:54.683941Z","shell.execute_reply.started":"2022-12-03T19:53:54.578777Z","shell.execute_reply":"2022-12-03T19:53:54.682955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM construction","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device, True)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:54.685264Z","iopub.execute_input":"2022-12-03T19:53:54.685866Z","iopub.status.idle":"2022-12-03T19:53:54.691551Z","shell.execute_reply.started":"2022-12-03T19:53:54.685827Z","shell.execute_reply":"2022-12-03T19:53:54.690537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_dimension = 100 # This is the dimension of the words we are using from GloVe\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                    word_dimension,\n                    weights=[embedding_matrix], # we set the weights to be the word vectors from GloVe\n                    input_length=seq_length,\n                    trainable=False)) # By setting trainable to False, we \"freeze\" the word embeddings.\nmodel.add(LSTM(60, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:54.693187Z","iopub.execute_input":"2022-12-03T19:53:54.693804Z","iopub.status.idle":"2022-12-03T19:53:54.840214Z","shell.execute_reply.started":"2022-12-03T19:53:54.693756Z","shell.execute_reply":"2022-12-03T19:53:54.839275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory = model.fit(x_train, y_train, batch_size=batch_size, epochs=20, validation_data=(x_test, y_test), callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2022-12-03T19:53:54.841531Z","iopub.execute_input":"2022-12-03T19:53:54.842301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_accuracy(history, title):\n    fig = plt.figure(figsize=(12, 6))\n    fig.suptitle(title)\n    ax = fig.add_subplot(1, 2, 1)\n    ax.plot(history.history[\"loss\"],'r-x', label=\"Train Loss\")\n    ax.plot(history.history[\"val_loss\"],'b-x', label=\"Validation Loss\")\n    ax.legend()\n    ax.set_title('cross_entropy loss')\n    ax.grid(True)\n\n\n    ax = fig.add_subplot(1, 2, 2)\n    ax.plot(history.history[\"accuracy\"],'r-x', label=\"Train Accuracy\")\n    ax.plot(history.history[\"val_accuracy\"],'b-x', label=\"Validation Accuracy\")\n    ax.legend()\n    ax.set_title('accuracy')\n    ax.grid(True)\n    \nplot_loss_accuracy(history, \"LSTM Model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}